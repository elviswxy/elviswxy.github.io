---
---

@misc{richemond2024offlineregularisedreinforcementlearning,
      title={Offline Regularised Reinforcement Learning for Large Language Models Alignment}, 
      author={Pierre Harvey Richemond and Yunhao Tang and Daniel Guo and Daniele Calandriello and Mohammad Gheshlaghi Azar and Rafael Rafailov and Bernardo Avila Pires and Eugene Tarassov and Lucas Spangher and Will Ellsworth and Aliaksei Severyn and Jonathan Mallinson and Lior Shani and Gil Shamir and Rishabh Joshi and Tianqi Liu and Remi Munos and Bilal Piot},
      year={2024},
      eprint={2405.19107},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.19107}, 
}

@misc{calandriello2024humanalignmentlargelanguage,
      title={Human Alignment of Large Language Models through Online Preference Optimisation}, 
      author={Daniele Calandriello and Daniel Guo and Remi Munos and Mark Rowland and Yunhao Tang and Bernardo Avila Pires and Pierre Harvey Richemond and Charline Le Lan and Michal Valko and Tianqi Liu and Rishabh Joshi and Zeyu Zheng and Bilal Piot},
      year={2024},
      eprint={2403.08635},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.08635}, 
}

@misc{geminiteam2024gemini15unlockingmultimodal,
      title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context}, 
      author={Gemini Team and Rishabh Joshi},
      year={2024},
      eprint={2403.05530},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.05530}, 
}

@misc{lipo2024,
      title={LiPO: Listwise Preference Optimization through Learning-to-Rank}, 
      author={Tianqi Liu and Zhen Qin and Junru Wu and Jiaming Shen and Misha Khalman and Rishabh Joshi and Yao Zhao and Mohammad Saleh and Simon Baumgartner and Jialu Liu and Peter J. Liu and Xuanhui Wang},
      year={2024},
      eprint={2402.01878},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.01878}, 
}

@misc{geminiteam2024geminifamilyhighlycapable,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={Gemini Team and Rishabh Joshi},
      year={2024},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.11805}, 
}

@misc{slicnli2023,
      title={Calibrating Likelihoods towards Consistency in Summarization Models}, 
      author={Polina Zablotskaia and Misha Khalman and Rishabh Joshi and Livio Baldini Soares and Shoshana Jakobovits and Joshua Maynez and Shashi Narayan},
      year={2023},
      eprint={2310.08764},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.08764}, 
}

@inproceedings{rso2024,
title={Statistical Rejection Sampling Improves Preference Optimization},
author={Tianqi Liu and Yao Zhao and Rishabh Joshi and Misha Khalman and Mohammad Saleh and Peter J Liu and Jialu Liu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=xbjSwwrQOe}
}

@misc{slichf2023,
      title={SLiC-HF: Sequence Likelihood Calibration with Human Feedback}, 
      author={Yao Zhao and Rishabh Joshi and Tianqi Liu and Misha Khalman and Mohammad Saleh and Peter J. Liu},
      year={2023},
      eprint={2305.10425},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.10425}, 
}

@inproceedings{slic2023,
title={Calibrating Sequence likelihood Improves Conditional Language Generation},
author={Yao Zhao and Mikhail Khalman and Rishabh Joshi and Shashi Narayan and Mohammad Saleh and Peter J Liu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=0qSOodKmJaN}
}

@inproceedings{joshi-etal-2023-unsupervised,
    title = "Unsupervised Keyphrase Extraction via Interpretable Neural Networks",
    author = "Joshi, Rishabh  and
      Balachandran, Vidhisha  and
      Saldanha, Emily  and
      Glenski, Maria  and
      Volkova, Svitlana  and
      Tsvetkov, Yulia",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2023",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-eacl.82",
    doi = "10.18653/v1/2023.findings-eacl.82",
    pages = "1107--1119",
    abstract = "Keyphrase extraction aims at automatically extracting a list of {``}important{''} phrases representing the key concepts in a document. Prior approaches for unsupervised keyphrase extraction resorted to heuristic notions of phrase importance via embedding clustering or graph centrality, requiring extensive domain expertise. Our work presents a simple alternative approach which defines keyphrases as document phrases that are salient for predicting the topic of the document. To this end, we propose INSPECT{---}an approach that uses self-explaining models for identifying influential keyphrases in a document by measuring the predictive impact of input phrases on the downstream task of the document topic classification. We show that this novel method not only alleviates the need for ad-hoc heuristics but also achieves state-of-the-art results in unsupervised keyphrase extraction in four datasets across two domains: scientific publications and news articles.",
}

@article{medicalel2021,
title = {Improving broad-coverage medical entity linking with semantic type prediction and large-scale datasets},
journal = {Journal of Biomedical Informatics},
volume = {121},
pages = {103880},
year = {2021},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103880},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421002094},
author = {Shikhar Vashishth and Denis Newman-Griffis and Rishabh Joshi and Ritam Dutt and Carolyn P. Rosé},
keywords = {Natural language processing, Information extraction, Medical concept normalization, Medical entity linking, Distant supervision, Entity typing},
abstract = {Objectives
Biomedical natural language processing tools are increasingly being applied for broad-coverage information extraction—extracting medical information of all types in a scientific document or a clinical note. In such broad-coverage settings, linking mentions of medical concepts to standardized vocabularies requires choosing the best candidate concepts from large inventories covering dozens of types. This study presents a novel semantic type prediction module for biomedical NLP pipelines and two automatically-constructed, large-scale datasets with broad coverage of semantic types.
Methods
We experiment with five off-the-shelf biomedical NLP toolkits on four benchmark datasets for medical information extraction from scientific literature and clinical notes. All toolkits adopt a staged approach of mention detection followed by two stages of medical entity linking: (1) generating a list of candidate concepts, and (2) picking the best concept among them. We introduce a semantic type prediction module to alleviate the problem of overgeneration of candidate concepts by filtering out irrelevant candidate concepts based on the predicted semantic type of a mention. We present MedType, a fully modular semantic type prediction model which we integrate into the existing NLP toolkits. To address the dearth of broad-coverage training data for medical information extraction, we further present WikiMed and PubMedDS, two large-scale datasets for medical entity linking.
Results
Semantic type filtering improves medical entity linking performance across all toolkits and datasets, often by several percentage points of F-1. Further, pretraining MedType on our novel datasets achieves state-of-the-art performance for semantic type prediction in biomedical text.
Conclusions
Semantic type prediction is a key part of building accurate NLP pipelines for broad-coverage information extraction from biomedical text. We make our source code and novel datasets publicly available to foster reproducible research.}
}

@inproceedings{ltiatcmu2020,
    title = "{LTI}at{CMU} at {S}em{E}val-2020 Task 11: Incorporating Multi-Level Features for Multi-Granular Propaganda Span Identification",
    author = "Khosla, Sopan  and
      Joshi, Rishabh  and
      Dutt, Ritam  and
      Black, Alan W  and
      Tsvetkov, Yulia",
    editor = "Herbelot, Aurelie  and
      Zhu, Xiaodan  and
      Palmer, Alexis  and
      Schneider, Nathan  and
      May, Jonathan  and
      Shutova, Ekaterina",
    booktitle = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
    month = dec,
    year = "2020",
    address = "Barcelona (online)",
    publisher = "International Committee for Computational Linguistics",
    url = "https://aclanthology.org/2020.semeval-1.230",
    doi = "10.18653/v1/2020.semeval-1.230",
    pages = "1756--1763",
    abstract = "In this paper we describe our submission for the task of Propaganda Span Identification in news articles. We introduce a BERT-BiLSTM based span-level propaganda classification model that identifies which token spans within the sentence are indicative of propaganda. The {''}multi-granular{''} model incorporates linguistic knowledge at various levels of text granularity, including word, sentence and document level syntactic, semantic and pragmatic affect features, which significantly improve model performance, compared to its language-agnostic variant. To facilitate better representation learning, we also collect a corpus of 10k news articles, and use it for fine-tuning the model. The final model is a majority-voting ensemble which learns different propaganda class boundaries by leveraging different subsets of incorporated knowledge.",
}

@article{misinformation2020,
   title={Analysing the Extent of Misinformation in Cancer Related Tweets}, volume={14}, url={https://ojs.aaai.org/index.php/ICWSM/article/view/7359}, DOI={10.1609/icwsm.v14i1.7359}, abstractNote={&lt;p&gt;Twitter has become one of the most sought after places to discuss a wide variety of topics, including medically relevant issues such as cancer. This helps spread awareness regarding the various causes, cures and prevention methods of cancer. However, no proper analysis has been performed, which discusses the validity of such claims. In this work, we aim to tackle the misinformation spread in such platforms. We collect and present a dataset regarding tweets which talk specifically about cancer and propose an attention-based deep learning model for automated detection of misinformation along with its spread. We then do a comparative analysis of the linguistic variation in the text corresponding to misinformation and truth. This analysis helps us gather relevant insights on various social aspects related to misinformed tweets.&lt;/p&gt;}, number={1}, journal={Proceedings of the International AAAI Conference on Web and Social Media}, author={Bal, Rakesh and Sinha, Sayan and Dutta, Swastika and Joshi, Rishabh and Ghosh, Sayan and Dutt, Ritam}, year={2020}, month={May}, pages={924-928}
  }

@inproceedings{joshi2021dialograph,
	title={DialoGraph: Incorporating Interpretable Strategy-Graph Networks into Negotiation Dialogues},
	author = 	{Joshi, Rishabh and Balachandran, Vidhisha and Vashishth, Shikhar and Black, Alan W and Tsvetkov, Yulia},
	booktitle={International Conference on Learning Representations},
	year={2021},
	url={https://openreview.net/forum?id=kDnal_bbb-E},
	abbr={ICLR21},
	pdf = {dialograph21/dialograph21.pdf},
	abstract = {To successfully negotiate a deal, it is not enough to communicate fluently: pragmatic planning of persuasive negotiation strategies is essential. While modern dialogue agents excel at generating fluent sentences, they still lack pragmatic grounding and cannot reason strategically. We present DialoGraph, a negotiation system that incorporates pragmatic strategies in a negotiation dialogue using graph neural networks. DialoGraph explicitly incorporates dependencies between sequences of strategies to enable improved and interpretable prediction of next optimal strategies, given the dialogue context. Our graph-based method outperforms prior state-of-the-art negotiation models both in the accuracy of strategy/dialogue act prediction and in the quality of downstream dialogue response generation. We qualitatively show further benefits of learned strategy-graphs in providing explicit associations between effective negotiation strategies over the course of the dialogue, leading to interpretable and strategic dialogues.},
}


@inproceedings{resper2021,
  title = 	{ResPer: Computationally Modelling Resisting Strategies in Persuasive Conversations.},
  author = 	{Dutt, Ritam and Sinha, Sayan and Joshi, Rishabh and Chakraborty, Surya Shekhar and Riggs, Meredith and Yan, Xinru and Bao, Haogang and Rose, Carolyn},
  booktitle = 	{Proceedings of the 2021 Conference on European Association for Computational Linguistics},
  year = 	{2021},
  abbr={EACL21},
}


@inproceedings{keepingup,
	title = "Keeping Up Appearances: Computational Modeling of Face Acts in Persuasion Oriented Discussions",
	author = "Dutt, Ritam  and Joshi, Rishabh  and Rose, Carolyn",
	booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/2020.emnlp-main.605",
	doi = "10.18653/v1/2020.emnlp-main.605",
	pages = "7473--7485",
	abstract = "The notion of face refers to the public self-image of an individual that emerges both from the individual{'}s own actions as well as from the interaction with others. Modeling face and understanding its state changes throughout a conversation is critical to the study of maintenance of basic human needs in and through interaction. Grounded in the politeness theory of Brown and Levinson (1978), we propose a generalized framework for modeling face acts in persuasion conversations, resulting in a reliable coding manual, an annotated corpus, and computational models. The framework reveals insights about differences in face act utilization between asymmetric roles in persuasion conversations. Using computational models, we are able to successfully identify face acts as well as predict a key conversational outcome (e.g. donation success). Finally, we model a latent representation of the conversational state to analyze the impact of predicted face acts on the probability of a positive conversational outcome and observe several correlations that corroborate previous findings.",
	abbr={EMNLP20},
}


@inproceedings{alexa2020,
	title={Tartan: A Two-Tiered Dialog Framework for Multi-Domain Social Chitchat},
	author={Chen, Fanglin and Chi Ta-Chung and Lyu, Shiyang and Gong, Jiachen and Parekh, Tanmay and Joshi, Rishabh and Kaushik, Anant and Rudnicky, Alexander},
	booktitle={Proceedings of the 3rd Alexa Prize 2019},
	year={2020},
	abbr=         {ALEXA20},
}


@ARTICLE{medtype2020,
       author = {{Vashishth}, Shikhar and {Joshi}, Rishabh and {Dutt}, Ritam and
         {Newman-Griffis}, Denis and {Rose}, Carolyn},
        title = "{MedType: Improving Medical Entity Linking with Semantic Type Prediction}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2020,
        month = may,
          eid = {arXiv:2005.00460},
        pages = {arXiv:2005.00460},
archivePrefix = {arXiv},
	abbr={arXiv},
       eprint = {2005.00460},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200500460V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{propaganda2020,
	title={LTIatCMU at SemEval-2020 Task 11: Incorporating Multi-Level Features for Multi-Granular Propaganda Span Identification},
	author ={Joshi<nobr><em>*</em></nobr>, Rishabh and Khosla*, Sopan and Dutt*, Ritam and Black, Alan W and Tsvetkov, Yulia},
	booktitle={Proceedings of the 14th Internation Workshop on Semantic Evaluation},
	address={Barcelona, Spain},
	year={2020},
	abbr={SEMEVAL20},
}


@inproceedings{amused2020,
  title = 	{AMUSED: A Multi-Stream Vector Representation Method for Use in Natural Dialogue},
  author = 	{Joshi<nobr><em>*</em></nobr>, Rishabh and Kumar*, Gaurav and Singh*, Jaspreet and Yenigalla, Promod},
  booktitle = 	{Proceedings of the 2020 International Conference on Language Resources and Evaluation},
  year = 	{2020},
  address = 	{Marseille, France},
  organization= 	{ELRA},
  abbr=         {LREC20},
    pdf = {amused20/amused20_paper.pdf},
    abstract = {The problem of building a coherent and non-monotonous conversational agent with proper discourse and coverage is still an area of open research. Current architectures only take care of semantic and contextual information for a given query and fail to completely account for syntactic and external knowledge which are crucial for generating responses in a chit-chat system. To overcome this problem, we propose an end to end multi-stream deep learning architecture which learns unified embeddings for query-response pairs by leveraging contextual information from memory networks and syntactic information by incorporating Graph Convolution Networks (GCN) over their dependency parse. A stream of this network also utilizes transfer learning by pre-training a bidirectional transformer to extract semantic representation for each input sentence and incorporates external knowledge through the the neighborhood of the entities from a Knowledge Base (KB). We benchmark these embeddings on next sentence prediction task and significantly improve upon the existing techniques. Furthermore, we use AMUSED to represent query and responses along with its context to develop a retrieval based conversational agent which has been validated by expert linguists to have comprehensive engagement with humans.},
    url =	{https://arxiv.org/abs/1912.10160}
}

@inproceedings{cancer2020,
  title = 	{Analysing the Extent of Misinformation in Cancer Related Tweets},
  author = 	{Bal*, Rakesh and Sinha*, Sayan and Dutta, Swastika and Joshi, Rishabh and Ghosh, Sayan and Dutt, Ritam},
  booktitle = 	{14th International Conference on Web and Social Media, 2020},
  year = 	{2020},
  address = 	{Atlanta, USA},
  organization= 	{AAAI},
  abbr=         {ICWSM20},
    pdf = {cancer20/cancer20_paper.pdf},
    abstract = {Twitter has become one of the most sought after places to discuss a wide variety of topics, including medically relevant issues such as cancer. This helps spread awareness regarding the various causes, cures and prevention methods of cancer.  However, no proper analysis has been performed, which discusses the validity of such claims. In this work, we aim to tackle the misinformation spread in such platforms. We collect and present a dataset regarding tweets which talk specifically about cancer and propose an attention-based deep learning model for automated detection of misinformation along with its spread. We then do a comparative analysis of the linguistic variation in the text corresponding to misinformation and truth. This analysis helps us gather relevant insights on various social aspects related to misinformed tweets.},
    url =	{https://arxiv.org/abs/2003.13657}
}




@inproceedings{reside2018,
  title = 	{RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information},
  author = 	{Vashishth, Shikhar and Joshi, Rishabh and Prayaga, Sai Suman and Bhattacharyya, Chiranjib and Talukdar, Partha},
  booktitle = 	{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  year = 	{2018},
  address = 	{Brussels, Belgium},
  organization= 	{Association for Computational Linguistics},
  abbr=         {EMNLP18},
    code = {https://github.com/malllabiisc/reside},
    video = {https://vimeo.com/305199302},
    supplement= {reside18/reside18_supplement.pdf},
    pdf = {reside18/reside18_paper.pdf},
    abstract = {Distantly-supervised Relation Extraction (RE) methods train an extractor by automatically aligning relation instances in a Knowledge Base (KB) with unstructured text. In addition to relation instances, KBs often contain other relevant side information, such as aliases of relations (e.g., founded and co-founded are aliases for the relation founderOfCompany). RE models usually ignore such readily available side information. In this paper, we propose RESIDE, a distantly-supervised neural relation extraction method which utilizes additional side information from KBs for improved relation extraction. It uses entity type and relation alias information for imposing soft constraints while predicting relations. RESIDE employs Graph Convolution Networks (GCN) to encode syntactic information from text and improves performance even when limited side information is available. Through extensive experiments on benchmark datasets, we demonstrate RESIDE’s effectiveness. We have made RESIDE’s source code available to encourage reproducible research.},
    address =	{Brussels, Belgium},
    pages =	{1257--1266},
    url =	{http://aclweb.org/anthology/D18-1157}
}
